{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define current working folder\n",
    "# curr_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tables from Instacart Kaggle dataset\n",
    "aisles = pd.read_csv(os.path.join(curr_dir, 'aisles.csv'))\n",
    "departments = pd.read_csv(os.path.join(curr_dir, 'departments.csv'))\n",
    "order_products_prior = pd.read_csv(os.path.join(curr_dir, 'order_products__prior.csv'))\n",
    "order_products_train = pd.read_csv(os.path.join(curr_dir, 'order_products__train.csv'))\n",
    "orders = pd.read_csv(os.path.join(curr_dir, 'orders.csv'))\n",
    "products = pd.read_csv(os.path.join(curr_dir, 'products.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the previous train/\"prior\" datasets used for training/test\n",
    "# we want to redo the train/test groups and not rely on the Kaggle \n",
    "# dataset creator's groups since we're going to drop data\n",
    "order_products = order_products_train\n",
    "order_products = order_products.append(order_products_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Check for duplicates in primary key of dataframes before merging:\n\nAISLES\nEmpty DataFrame\nColumns: [aisle_id, aisle]\nIndex: []\n\nDEPARTMENTS\nEmpty DataFrame\nColumns: [department_id, department]\nIndex: []\n\nORDER_PRODUCTS_PRIOR\nEmpty DataFrame\nColumns: [order_id, product_id, add_to_cart_order, reordered]\nIndex: []\n\nORDER_PRODUCTS_TRAIN\nEmpty DataFrame\nColumns: [order_id, product_id, add_to_cart_order, reordered]\nIndex: []\n\nORDER_DUPS\nEmpty DataFrame\nColumns: [order_id, user_id, eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_order]\nIndex: []\n\nPRODUCT_DUPS\nEmpty DataFrame\nColumns: [product_id, product_name, aisle_id, department_id]\nIndex: []\n\nORDER_PRODUCT_DUPS\nEmpty DataFrame\nColumns: [order_id, product_id, add_to_cart_order, reordered]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# check for duplicated primary keys\n",
    "aisles_dups = aisles[aisles.duplicated(['aisle_id'])]\n",
    "departments_dups = departments[departments.duplicated(['department_id'])]\n",
    "order_products_prior_dups = order_products_prior[order_products_prior.duplicated(['order_id','product_id'])]\n",
    "order_products_train_dups = order_products_train[order_products_train.duplicated(['order_id','product_id'])]\n",
    "orders_dups = orders[orders.duplicated(['order_id'])]\n",
    "products_dups = products[products.duplicated(['product_id'])]\n",
    "order_products_dups = order_products[order_products.duplicated(['order_id','product_id'])]\n",
    "\n",
    "print('Check for duplicates in primary key of dataframes before merging:')\n",
    "print('\\nAISLES')\n",
    "print(aisles_dups)\n",
    "print('\\nDEPARTMENTS')\n",
    "print(departments_dups)\n",
    "print('\\nORDER_PRODUCTS_PRIOR')\n",
    "print(order_products_prior_dups)\n",
    "print('\\nORDER_PRODUCTS_TRAIN')\n",
    "print(order_products_train_dups)\n",
    "print('\\nORDER_DUPS')\n",
    "print(orders_dups)\n",
    "print('\\nPRODUCT_DUPS')\n",
    "print(products_dups)\n",
    "print('\\nORDER_PRODUCT_DUPS')\n",
    "print(order_products_dups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the tables together because there are no duplicates on primary keys \n",
    "all_merged_data = pd.merge(order_products, products, how='left', on=['product_id', 'product_id'])\n",
    "all_merged_data = pd.merge(all_merged_data, orders, how='left', on=['order_id', 'order_id'])\n",
    "all_merged_data = pd.merge(all_merged_data, aisles, how='left', on=['aisle_id', 'aisle_id'])\n",
    "all_merged_data = pd.merge(all_merged_data, departments, how='left', on=['department_id', 'department_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "27.66% of product codes have 20 or fewer sales accounting for 0.42% of all sales\n41.28% of product codes have 40 or fewer sales accounting for 1.01% of all sales\n49.35% of product codes have 60 or fewer sales accounting for 1.6% of all sales\n54.97% of product codes have 80 or fewer sales accounting for 2.18% of all sales\n58.98% of product codes have 100 or fewer sales accounting for 2.71% of all sales\n62.22% of product codes have 120 or fewer sales accounting for 3.23% of all sales\n64.88% of product codes have 140 or fewer sales accounting for 3.74% of all sales\n67.15% of product codes have 160 or fewer sales accounting for 4.24% of all sales\n68.94% of product codes have 180 or fewer sales accounting for 4.69% of all sales\n70.53% of product codes have 200 or fewer sales accounting for 5.14% of all sales\n\n\nTOP PRODUCTS (BY ORDERS CONTAINING PRODUCT)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       product_id                      product_name   count\n",
       "0           24852                            Banana  491291\n",
       "1           13176            Bag of Organic Bananas  394930\n",
       "2           21137              Organic Strawberries  275577\n",
       "3           21903              Organic Baby Spinach  251705\n",
       "4           47209              Organic Hass Avocado  220877\n",
       "...           ...                               ...     ...\n",
       "14637       43831               Extra Dry Champagne     201\n",
       "14638        5247                         Zero Cola     201\n",
       "14639       41900        Bagel Chips Toasted Garlic     201\n",
       "14640       17142  Tandoori Inspired Spiced Chicken     201\n",
       "14641       16975                          Bow Ties     201\n",
       "\n",
       "[14642 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>product_name</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24852</td>\n      <td>Banana</td>\n      <td>491291</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13176</td>\n      <td>Bag of Organic Bananas</td>\n      <td>394930</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21137</td>\n      <td>Organic Strawberries</td>\n      <td>275577</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21903</td>\n      <td>Organic Baby Spinach</td>\n      <td>251705</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>47209</td>\n      <td>Organic Hass Avocado</td>\n      <td>220877</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14637</th>\n      <td>43831</td>\n      <td>Extra Dry Champagne</td>\n      <td>201</td>\n    </tr>\n    <tr>\n      <th>14638</th>\n      <td>5247</td>\n      <td>Zero Cola</td>\n      <td>201</td>\n    </tr>\n    <tr>\n      <th>14639</th>\n      <td>41900</td>\n      <td>Bagel Chips Toasted Garlic</td>\n      <td>201</td>\n    </tr>\n    <tr>\n      <th>14640</th>\n      <td>17142</td>\n      <td>Tandoori Inspired Spiced Chicken</td>\n      <td>201</td>\n    </tr>\n    <tr>\n      <th>14641</th>\n      <td>16975</td>\n      <td>Bow Ties</td>\n      <td>201</td>\n    </tr>\n  </tbody>\n</table>\n<p>14642 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# CUT DOWN ON PRODUCTS\n",
    "# Need to reduce the number of products in the user/products matrix for recommendations (123k users x 49k products = ~110GB of RAM required for float distance calculations)\n",
    "\n",
    "# get the products and the number of orders they were sold in\n",
    "product_dist = all_merged_data.groupby(['product_id','product_name']).size().sort_values(ascending=False).reset_index()\n",
    "product_dist.columns = ['product_id','product_name','count']\n",
    "\n",
    "# loop through the count of products by order they were sold in to see\n",
    "# information about products with very low sales (like % of volume) \n",
    "for i in range(1,11):\n",
    "    threshold = 20 * i\n",
    "\n",
    "    pct_of_prod_ids_under_threshold = 100 * product_dist['product_id'].loc[product_dist['count'] <= threshold].count() / product_dist['product_id'].count()\n",
    "    \n",
    "    pct_of_prod_volume_sold_under_threshold = 100 * product_dist['count'].loc[product_dist['count'] <= threshold].sum() / product_dist['count'].sum()\n",
    "\n",
    "    print(str(round(pct_of_prod_ids_under_threshold,2)) + '% of product codes have ' + str(threshold) + \\\n",
    "            ' or fewer sales accounting for ' + str(round(pct_of_prod_volume_sold_under_threshold,2)) + '% of all sales')\n",
    "\n",
    "# choose product sales volume threshold for inclusion in model\n",
    "threshold = 200\n",
    "\n",
    "# drop low-sale-count products from all_merged data (likely not enough purchases to make good recommendations, )\n",
    "product_dist.drop(product_dist['product_id'].loc[product_dist['count'] <= threshold].index, inplace=True)\n",
    "\n",
    "print('\\n\\nTOP PRODUCTS (BY ORDERS CONTAINING PRODUCT)')\n",
    "product_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0% of users have 1 or fewer orders, or 0.0% of all orders (0.0% of all product order volume).\n0.0% of users have 2 or fewer orders, or 0.0% of all orders (0.0% of all product order volume).\n4.21% of users have 3 or fewer orders, or 0.78% of all orders (4.13% of all product order volume).\n15.1% of users have 4 or fewer orders, or 3.46% of all orders (15.08% of all product order volume).\n23.96% of users have 5 or fewer orders, or 6.19% of all orders (23.85% of all product order volume).\n31.39% of users have 6 or fewer orders, or 8.94% of all orders (31.34% of all product order volume).\n37.79% of users have 7 or fewer orders, or 11.7% of all orders (37.67% of all product order volume).\n43.16% of users have 8 or fewer orders, or 14.35% of all orders (42.89% of all product order volume).\n47.9% of users have 9 or fewer orders, or 16.98% of all orders (47.69% of all product order volume).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        user_id  count\n",
       "0        170771    100\n",
       "1        205483    100\n",
       "2         96192    100\n",
       "3        137255    100\n",
       "4         65141    100\n",
       "...         ...    ...\n",
       "156800   169628      6\n",
       "156801    58070      6\n",
       "156802   124533      6\n",
       "156803   100531      6\n",
       "156804    61386      6\n",
       "\n",
       "[156805 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>170771</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>205483</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>96192</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>137255</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>65141</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>156800</th>\n      <td>169628</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>156801</th>\n      <td>58070</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>156802</th>\n      <td>124533</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>156803</th>\n      <td>100531</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>156804</th>\n      <td>61386</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>156805 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# CUT DOWN ON USERS\n",
    "# Need to reduce the number of users in the user/products matrix for recommendations (123k users x 49k products = ~110GB of RAM required for float distance calculations)\n",
    "\n",
    "# step 1 - get the number of products by order and user\n",
    "user_order_dist = all_merged_data.groupby(['user_id','order_id']).agg(count=('product_id','count')).sort_values(by='count',ascending=False).reset_index()\n",
    "user_order_dist.columns = ['user_id','order_id','count']\n",
    "\n",
    "# step 2 - get the number of products (count them for each order) by user\n",
    "user_order_prod_count_dist = user_order_dist.groupby(['user_id']).agg(count=('count','sum')).reset_index().sort_values(by='count',ascending=False)\n",
    "user_order_prod_count_dist\n",
    "\n",
    "# step 2 - get the number of orders by user (there are between 3 and 100 orders per user in this dataset)\n",
    "user_dist = user_order_dist.groupby(['user_id']).size().sort_values(ascending=False).reset_index()\n",
    "user_dist.columns = ['user_id','count']\n",
    "\n",
    "# loop through order counts by user (3 to 10) and show product sold (count them for each order)\n",
    "# to see where to cut the data off\n",
    "for i in range(1,10):\n",
    "    threshold = i\n",
    "\n",
    "    pct_of_user_ids_under_threshold = 100 * user_dist['user_id'].loc[user_dist['count'] <= threshold].count() / user_dist['user_id'].count()\n",
    "    pct_of_user_volume_sold_under_threshold = 100 * user_dist['count'].loc[user_dist['count'] <= threshold].sum() / user_dist['count'].sum()\n",
    "    pct_of_sales_volume_sold_under_threshold = 100 * user_order_prod_count_dist['count'].loc[user_dist['count'] <= threshold].sum() / user_order_prod_count_dist['count'].sum()\n",
    "\n",
    "    print(str(round(pct_of_user_ids_under_threshold,2)) + '% of users have ' + str(threshold) + \\\n",
    "            ' or fewer orders, or ' + str(round(pct_of_user_volume_sold_under_threshold,2)) + \\\n",
    "            '% of all orders (' + str(round(pct_of_sales_volume_sold_under_threshold,2)) +'% of all product order volume).')\n",
    "\n",
    "# choose product sales volume threshold for inclusion in model\n",
    "threshold = 5\n",
    "\n",
    "# drop low-sale-count products from all_merged data (likely not enough purchases to make good recommendations, )\n",
    "user_dist.drop(user_dist['user_id'].loc[user_dist['count'] <= threshold].index, inplace=True)\n",
    "user_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE USERS AND PRODUCTS\n",
    "\n",
    "# inner join the all_merged_data with the product_dist and user_dist to keep the desired users and products\n",
    "all_merged_data = pd.merge(all_merged_data, product_dist, how='inner', on='product_id')\n",
    "all_merged_data = pd.merge(all_merged_data, user_dist, how='inner', on='user_id')\n",
    "\n",
    "all_merged_data\n",
    "\n",
    "# save the merged data as a csv (so that it can be imported later without needing to re-merge)\n",
    "all_merged_data.to_csv(os.path.join(curr_dir,'all_merged_data.csv'), sep=',', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/validate/test datasets\n",
    "\n",
    "# randomly select 60%, 20% and 20% of user_ids for train/validate/test datasets\n",
    "user_ids = all_merged_data[['user_id']].drop_duplicates()\n",
    "\n",
    "# use a random seed (111) so that the random numbers generated are the same even if this code is re-run\n",
    "user_ids_train, user_ids_validate, user_ids_test = np.split(user_ids.sample(frac=1, random_state=111), \n",
    "                                                            [int(.6*len(user_ids)), int(.8*len(user_ids))])\n",
    "\n",
    "# delete user_ids\n",
    "del user_ids\n",
    "\n",
    "# create the train, test and validation datasets\n",
    "train = user_ids_train.merge(all_merged_data, how='inner', on='user_id')\n",
    "test = user_ids_test.merge(all_merged_data, how='inner', on='user_id')\n",
    "validate = user_ids_validate.merge(all_merged_data, how='inner', on='user_id')\n",
    "\n",
    "# save the test and validate dataframes to csv (will delete the dataframes later in this code and import for future use - keep memory usage low)\n",
    "test.to_csv(os.path.join(curr_dir,'model_test.csv'), sep=',', encoding='utf-8', index=False)\n",
    "validate.to_csv(os.path.join(curr_dir,'model_validate.csv'), sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "# test and validate datasets are deleted later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          user_id  order_id  product_id  add_to_cart_order  reordered  \\\n",
       "0          121770     37902       11109                 26          0   \n",
       "1          121770    255629       11109                  5          1   \n",
       "2          121770    545631       11109                  7          1   \n",
       "3          121770    881845       11109                  7          1   \n",
       "4          121770    900923       11109                 12          1   \n",
       "...           ...       ...         ...                ...        ...   \n",
       "18155823   130415   2533989       44971                 10          0   \n",
       "18155824   130415   1780990       47301                 16          1   \n",
       "18155825   130415   1860568       47301                  4          1   \n",
       "18155826   130415   2533989       47301                  1          1   \n",
       "18155827   130415   2566635       47301                  6          0   \n",
       "\n",
       "                                         product_name_x  aisle_id  \\\n",
       "0         Organic 4% Milk Fat Whole Milk Cottage Cheese       108   \n",
       "1         Organic 4% Milk Fat Whole Milk Cottage Cheese       108   \n",
       "2         Organic 4% Milk Fat Whole Milk Cottage Cheese       108   \n",
       "3         Organic 4% Milk Fat Whole Milk Cottage Cheese       108   \n",
       "4         Organic 4% Milk Fat Whole Milk Cottage Cheese       108   \n",
       "...                                                 ...       ...   \n",
       "18155823                     1st Foods Baby Food- Pears        92   \n",
       "18155824                    Very Young Small Sweet Peas        81   \n",
       "18155825                    Very Young Small Sweet Peas        81   \n",
       "18155826                    Very Young Small Sweet Peas        81   \n",
       "18155827                    Very Young Small Sweet Peas        81   \n",
       "\n",
       "          department_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
       "0                    16    prior             1          0                  8   \n",
       "1                    16    prior             3          1                 11   \n",
       "2                    16    prior            43          6                 11   \n",
       "3                    16    prior            33          5                  9   \n",
       "4                    16    prior            42          0                  6   \n",
       "...                 ...      ...           ...        ...                ...   \n",
       "18155823             18    prior             8          1                 22   \n",
       "18155824             15    prior            10          1                 15   \n",
       "18155825             15    prior             5          3                 22   \n",
       "18155826             15    prior             8          1                 22   \n",
       "18155827             15    prior             2          0                  2   \n",
       "\n",
       "          days_since_prior_order                     aisle    department  \\\n",
       "0                            NaN      other creams cheeses    dairy eggs   \n",
       "1                            8.0      other creams cheeses    dairy eggs   \n",
       "2                           13.0      other creams cheeses    dairy eggs   \n",
       "3                           28.0      other creams cheeses    dairy eggs   \n",
       "4                           14.0      other creams cheeses    dairy eggs   \n",
       "...                          ...                       ...           ...   \n",
       "18155823                    18.0         baby food formula        babies   \n",
       "18155824                    17.0  canned jarred vegetables  canned goods   \n",
       "18155825                     5.0  canned jarred vegetables  canned goods   \n",
       "18155826                    18.0  canned jarred vegetables  canned goods   \n",
       "18155827                    30.0  canned jarred vegetables  canned goods   \n",
       "\n",
       "                                         product_name_y  count_x  count_y  \n",
       "0         Organic 4% Milk Fat Whole Milk Cottage Cheese     4616       43  \n",
       "1         Organic 4% Milk Fat Whole Milk Cottage Cheese     4616       43  \n",
       "2         Organic 4% Milk Fat Whole Milk Cottage Cheese     4616       43  \n",
       "3         Organic 4% Milk Fat Whole Milk Cottage Cheese     4616       43  \n",
       "4         Organic 4% Milk Fat Whole Milk Cottage Cheese     4616       43  \n",
       "...                                                 ...      ...      ...  \n",
       "18155823                     1st Foods Baby Food- Pears      238       13  \n",
       "18155824                    Very Young Small Sweet Peas      266       13  \n",
       "18155825                    Very Young Small Sweet Peas      266       13  \n",
       "18155826                    Very Young Small Sweet Peas      266       13  \n",
       "18155827                    Very Young Small Sweet Peas      266       13  \n",
       "\n",
       "[18155828 rows x 18 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>order_id</th>\n      <th>product_id</th>\n      <th>add_to_cart_order</th>\n      <th>reordered</th>\n      <th>product_name_x</th>\n      <th>aisle_id</th>\n      <th>department_id</th>\n      <th>eval_set</th>\n      <th>order_number</th>\n      <th>order_dow</th>\n      <th>order_hour_of_day</th>\n      <th>days_since_prior_order</th>\n      <th>aisle</th>\n      <th>department</th>\n      <th>product_name_y</th>\n      <th>count_x</th>\n      <th>count_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>121770</td>\n      <td>37902</td>\n      <td>11109</td>\n      <td>26</td>\n      <td>0</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>108</td>\n      <td>16</td>\n      <td>prior</td>\n      <td>1</td>\n      <td>0</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>other creams cheeses</td>\n      <td>dairy eggs</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>4616</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>121770</td>\n      <td>255629</td>\n      <td>11109</td>\n      <td>5</td>\n      <td>1</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>108</td>\n      <td>16</td>\n      <td>prior</td>\n      <td>3</td>\n      <td>1</td>\n      <td>11</td>\n      <td>8.0</td>\n      <td>other creams cheeses</td>\n      <td>dairy eggs</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>4616</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>121770</td>\n      <td>545631</td>\n      <td>11109</td>\n      <td>7</td>\n      <td>1</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>108</td>\n      <td>16</td>\n      <td>prior</td>\n      <td>43</td>\n      <td>6</td>\n      <td>11</td>\n      <td>13.0</td>\n      <td>other creams cheeses</td>\n      <td>dairy eggs</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>4616</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>121770</td>\n      <td>881845</td>\n      <td>11109</td>\n      <td>7</td>\n      <td>1</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>108</td>\n      <td>16</td>\n      <td>prior</td>\n      <td>33</td>\n      <td>5</td>\n      <td>9</td>\n      <td>28.0</td>\n      <td>other creams cheeses</td>\n      <td>dairy eggs</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>4616</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>121770</td>\n      <td>900923</td>\n      <td>11109</td>\n      <td>12</td>\n      <td>1</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>108</td>\n      <td>16</td>\n      <td>prior</td>\n      <td>42</td>\n      <td>0</td>\n      <td>6</td>\n      <td>14.0</td>\n      <td>other creams cheeses</td>\n      <td>dairy eggs</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>4616</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18155823</th>\n      <td>130415</td>\n      <td>2533989</td>\n      <td>44971</td>\n      <td>10</td>\n      <td>0</td>\n      <td>1st Foods Baby Food- Pears</td>\n      <td>92</td>\n      <td>18</td>\n      <td>prior</td>\n      <td>8</td>\n      <td>1</td>\n      <td>22</td>\n      <td>18.0</td>\n      <td>baby food formula</td>\n      <td>babies</td>\n      <td>1st Foods Baby Food- Pears</td>\n      <td>238</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>18155824</th>\n      <td>130415</td>\n      <td>1780990</td>\n      <td>47301</td>\n      <td>16</td>\n      <td>1</td>\n      <td>Very Young Small Sweet Peas</td>\n      <td>81</td>\n      <td>15</td>\n      <td>prior</td>\n      <td>10</td>\n      <td>1</td>\n      <td>15</td>\n      <td>17.0</td>\n      <td>canned jarred vegetables</td>\n      <td>canned goods</td>\n      <td>Very Young Small Sweet Peas</td>\n      <td>266</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>18155825</th>\n      <td>130415</td>\n      <td>1860568</td>\n      <td>47301</td>\n      <td>4</td>\n      <td>1</td>\n      <td>Very Young Small Sweet Peas</td>\n      <td>81</td>\n      <td>15</td>\n      <td>prior</td>\n      <td>5</td>\n      <td>3</td>\n      <td>22</td>\n      <td>5.0</td>\n      <td>canned jarred vegetables</td>\n      <td>canned goods</td>\n      <td>Very Young Small Sweet Peas</td>\n      <td>266</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>18155826</th>\n      <td>130415</td>\n      <td>2533989</td>\n      <td>47301</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Very Young Small Sweet Peas</td>\n      <td>81</td>\n      <td>15</td>\n      <td>prior</td>\n      <td>8</td>\n      <td>1</td>\n      <td>22</td>\n      <td>18.0</td>\n      <td>canned jarred vegetables</td>\n      <td>canned goods</td>\n      <td>Very Young Small Sweet Peas</td>\n      <td>266</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>18155827</th>\n      <td>130415</td>\n      <td>2566635</td>\n      <td>47301</td>\n      <td>6</td>\n      <td>0</td>\n      <td>Very Young Small Sweet Peas</td>\n      <td>81</td>\n      <td>15</td>\n      <td>prior</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>30.0</td>\n      <td>canned jarred vegetables</td>\n      <td>canned goods</td>\n      <td>Very Young Small Sweet Peas</td>\n      <td>266</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n<p>18155828 rows Ã— 18 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are gaps in the user_ids and product_ids due to the \n",
    "# sampling so create a mapping table to assign sequential ids,\n",
    "# but be able to map back to the original users/products later\n",
    "\n",
    "train_users_map = train[['user_id']].drop_duplicates().reset_index(drop=True)\n",
    "train_users_map['model_user_id'] = train_users_map.index\n",
    "\n",
    "train_products_map = train[['product_id']].drop_duplicates().reset_index(drop=True)\n",
    "train_products_map['model_product_id'] = train_products_map.index\n",
    "\n",
    "# update train_users\n",
    "train = train.merge(train_users_map, how='inner', on='user_id')\n",
    "train = train.merge(train_products_map, how='inner', on='product_id')\n",
    "train.drop(['user_id','product_id'], axis=1, inplace=True)\n",
    "\n",
    "# export the train_users_map and train_products_map to csv (need this later to get the product information)\n",
    "train_users_map.to_csv(os.path.join(curr_dir,'train_users_map.csv'), sep=',', encoding='utf-8', index=False)\n",
    "train_products_map.to_csv(os.path.join(curr_dir,'train_products_map.csv'), sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "# users have multiple orders, so need to collapse the orders \n",
    "# and sum the reordered binary indicator field by grouping train\n",
    "# on user_id and product_id and summing reordered \n",
    "# (reordered + 1 is the total number of times a user ordered a product)\n",
    "train = train.groupby(['model_user_id','model_product_id']).agg(reordered_count=('reordered','sum')).reset_index()\n",
    "\n",
    "# save the train_user_maps and train_product_maps to csv (will delete the dataframes later in this code and import for future use - keep memory usage low)\n",
    "train_users_map.to_csv(os.path.join(curr_dir,'model_train_user_map.csv'), sep=',', encoding='utf-8', index=False)\n",
    "train_products_map.to_csv(os.path.join(curr_dir,'model_train_product_map.csv'), sep=',', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         model_user_id  model_product_id  reordered_count\n",
       "0                    0                 0               19\n",
       "1                    0                 1                7\n",
       "2                    0                 2               25\n",
       "3                    0                 3                9\n",
       "4                    0                 4               10\n",
       "...                ...               ...              ...\n",
       "6877928          94078              9277                6\n",
       "6877929          94078             10043                0\n",
       "6877930          94078             10523                2\n",
       "6877931          94078             10850                0\n",
       "6877932          94078             11796                3\n",
       "\n",
       "[6877933 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_user_id</th>\n      <th>model_product_id</th>\n      <th>reordered_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6877928</th>\n      <td>94078</td>\n      <td>9277</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6877929</th>\n      <td>94078</td>\n      <td>10043</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6877930</th>\n      <td>94078</td>\n      <td>10523</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6877931</th>\n      <td>94078</td>\n      <td>10850</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6877932</th>\n      <td>94078</td>\n      <td>11796</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>6877933 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 recommendation engines can be created from the user/product data\n",
    "\n",
    "# User-user collaborative recommendation\n",
    "#    A product is recommended to a user based on how similar all their purchases are to other users\n",
    "#         e.g. user A buys frozen berries, lactose-free milk, and lactose-free yoghurt, and flax seeds\n",
    "#              user B buys frozen berries, lactose-free milk, and lactose-free yoghurt \n",
    "#                     >>>> flax seeds will be recommended\n",
    "#    this is looking for combinations of products across users and matching similar users to recommend products\n",
    "\n",
    "# Product-product collaborative recommendation\n",
    "#    A product is recommended to a user based on a commonality of items they've purchased/in their cart\n",
    "#         e.g. user A buys bacon, ham and eggs;\n",
    "#              user B buys bacon, eggs; \n",
    "#              user C buys eggs \n",
    "#                     >>>> then bacon will be the item recommended\n",
    "#    this is looking for combinations of products (regardless of other user purchases) \n",
    "#    and matching products often bought together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the user-product matrix for both item-item and user-iser recommendations\n",
    "\n",
    "# get the dimensions (users by products)\n",
    "n_users = train.model_user_id.unique().shape[0]\n",
    "n_products = train.model_product_id.unique().shape[0]\n",
    "\n",
    "# matrix of ordered products\n",
    "# data_matrix = np.zeros((n_users, n_products), dtype=int)\n",
    "data_matrix = np.zeros((n_users, n_products), dtype=int)\n",
    "\n",
    "\n",
    "# iterate through the tuples of the dataframe\n",
    "for line in train.itertuples():\n",
    "    # line[0] is the index, line[1] is the model_user_ids, line[2] are the model_product_ids, line[3] is the reordered_count (add one for total orders)\n",
    "    data_matrix[line[1]-1, line[2]-1] = line[3]+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(94079, 14642)"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "# save the numpy array as a binary file (can be loaded into python directly)\n",
    "np.save(os.path.join(curr_dir, 'train_user_product_matrix.npy'), data_matrix=data_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataframe *aisles* does not exist.\ndataframe *departments* does not exist.\ndataframe *order_products_prior* does not exist.\ndataframe *order_products_train* does not exist.\ndataframe *orders* does not exist.\ndataframe *products* does not exist.\ndataframe *aisles_dups* does not exist.\ndataframe *departments_dups* does not exist.\ndataframe *order_products_prior_dups* does not exist.\ndataframe *order_products_train_dups* does not exist.\ndataframe *orders_dups* does not exist.\ndataframe *products_dups* does not exist.\ndataframe *order_products_dups* does not exist.\ndataframe *product_dist* does not exist.\ndataframe *user_dist* does not exist.\ndataframe *user_order_prod_count_dist* does not exist.\ndataframe *all_merged_data* does not exist.\ndataframe *user_order_prod_count_dist* does not exist.\n"
     ]
    }
   ],
   "source": [
    "# DROP ALL THE PAST DATAFRAMES THAT AREN'T NEEDED\n",
    "# NEED TO DO THIS TO LOWER MEMORY USAGE AS MUCH AS POSSIBLE\n",
    "try:\n",
    "    del aisles\n",
    "except:\n",
    "    print('dataframe *aisles* does not exist.')\n",
    "\n",
    "try:\n",
    "    del departments\n",
    "except:\n",
    "    print('dataframe *departments* does not exist.')\n",
    "\n",
    "try:\n",
    "    del order_products_prior\n",
    "except:\n",
    "    print('dataframe *order_products_prior* does not exist.')\n",
    "\n",
    "try:\n",
    "    del order_products_train\n",
    "except:\n",
    "    print('dataframe *order_products_train* does not exist.')\n",
    "\n",
    "try:\n",
    "    del orders\n",
    "except:\n",
    "    print('dataframe *orders* does not exist.')\n",
    "\n",
    "try:\n",
    "    del products\n",
    "except:\n",
    "    print('dataframe *products* does not exist.')\n",
    "\n",
    "try:\n",
    "    del aisles_dups\n",
    "except:\n",
    "    print('dataframe *aisles_dups* does not exist.')\n",
    "\n",
    "try:\n",
    "    del departments_dups\n",
    "except:\n",
    "    print('dataframe *departments_dups* does not exist.')\n",
    "\n",
    "try:\n",
    "    del order_products_prior_dups\n",
    "except:\n",
    "    print('dataframe *order_products_prior_dups* does not exist.')\n",
    "\n",
    "try:\n",
    "    del order_products_train_dups\n",
    "except:\n",
    "    print('dataframe *order_products_train_dups* does not exist.')\n",
    "\n",
    "try:\n",
    "    del orders_dups\n",
    "except:\n",
    "    print('dataframe *orders_dups* does not exist.')\n",
    "\n",
    "try:\n",
    "    del products_dups\n",
    "except:\n",
    "    print('dataframe *products_dups* does not exist.')\n",
    "\n",
    "try:\n",
    "    del order_products_dups\n",
    "except:\n",
    "    print('dataframe *order_products_dups* does not exist.')\n",
    "\n",
    "try:\n",
    "    del product_dist\n",
    "except:\n",
    "    print('dataframe *product_dist* does not exist.')\n",
    "\n",
    "try:\n",
    "    del user_dist\n",
    "except:\n",
    "    print('dataframe *user_dist* does not exist.')\n",
    "\n",
    "try:\n",
    "    del user_order_prod_count_dist\n",
    "except:\n",
    "    print('dataframe *user_order_prod_count_dist* does not exist.')\n",
    "\n",
    "try:\n",
    "    del all_merged_data\n",
    "except:\n",
    "    print('dataframe *all_merged_data* does not exist.')\n",
    "\n",
    "try:\n",
    "    del user_order_prod_count_dist\n",
    "except:\n",
    "    print('dataframe *user_order_prod_count_dist* does not exist.')\n",
    "\n"
   ]
  },
  {
   "source": [
    "**************** ONCE YOU HAVE THE MATRIX SAVED, RUN CODE FROM HERE ****************************************************************************************"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TO LOAD FROM THIS POINT\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "# # curr_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "# load the binary numpy array back into python \n",
    "with np.load(os.path.join(curr_dir, 'train_user_product_matrix.npy')) as data:\n",
    "    data_matrix = data['data_matrix']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 65.9 GiB for an array with shape (94079, 94079) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-e327e822f2ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muser_similarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cosine'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   1773\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_distances\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \"\"\"\n\u001b[0;32m    830\u001b[0m     \u001b[1;31m# 1.0 - cosine_similarity(X, Y) without copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m     \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m     K = safe_sparse_dot(X_normalized, Y_normalized.T,\n\u001b[0m\u001b[0;32m   1187\u001b[0m                         dense_output=dense_output)\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 65.9 GiB for an array with shape (94079, 94079) and data type float64"
     ]
    }
   ],
   "source": [
    "# Create the user-user distances between products (to match users and recommend the best product)\n",
    "user_similarity = pairwise_distances(data_matrix, metric='cosine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.3 GiB for an array with shape (14642, 94079) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-114e5549b63d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# from sklearn.metrics.pairwise import pairwise_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mproduct_similarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cosine'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   1773\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_distances\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \"\"\"\n\u001b[0;32m    830\u001b[0m     \u001b[1;31m# 1.0 - cosine_similarity(X, Y) without copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m     \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;31m# to avoid recursive import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         X = Y = check_array(X, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[0m\u001b[0;32m    141\u001b[0m                             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                             estimator=estimator)\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    597\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.3 GiB for an array with shape (14642, 94079) and data type float64"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics.pairwise import pairwise_distances\n",
    "product_similarity = pairwise_distances(data_matrix.T, metric='cosine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        #We use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prediction = predict(data_matrix, user_similarity, type='user')\n",
    "item_prediction = predict(data_matrix, item_similarity, type='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}